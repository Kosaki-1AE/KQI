{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/C8QcIxd4Mq5O0FyleMoV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkRyhehdbZOJ","executionInfo":{"status":"ok","timestamp":1766010569951,"user_tz":-540,"elapsed":4379,"user":{"displayName":"零一真","userId":"13547062280887801150"}},"outputId":"d57db853-8eb1-4612-e71a-4f156f017572"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"]}],"source":["!pip install torch"]},{"cell_type":"code","source":["import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# ----------------------------\n","# Positional Encoding\n","# ----------------------------\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","\n","        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # (max_len, 1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)  # even\n","        pe[:, 1::2] = torch.cos(position * div_term)  # odd\n","\n","        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (batch, seq_len, d_model)\n","        \"\"\"\n","        seq_len = x.size(1)\n","        x = x + self.pe[:, :seq_len, :]\n","        return self.dropout(x)\n","\n","\n","# ----------------------------\n","# Masks\n","# ----------------------------\n","def generate_square_subsequent_mask(sz: int, device=None) -> torch.Tensor:\n","    \"\"\"\n","    Causal mask for decoder self-attn.\n","    Returns (sz, sz) where upper triangle is -inf.\n","    \"\"\"\n","    device = device or torch.device(\"cpu\")\n","    mask = torch.full((sz, sz), float(\"-inf\"), device=device)\n","    mask = torch.triu(mask, diagonal=1)\n","    return mask\n","\n","def make_padding_mask(tokens: torch.Tensor, pad_id: int) -> torch.Tensor:\n","    \"\"\"\n","    tokens: (batch, seq_len)\n","    returns: (batch, seq_len) True where PAD\n","    \"\"\"\n","    return tokens.eq(pad_id)\n","\n","\n","# ----------------------------\n","# Full Transformer (Embedding -> Encoder -> Decoder -> Linear)\n","# ----------------------------\n","class TransformerSeq2Seq(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size: int,\n","        tgt_vocab_size: int,\n","        d_model: int = 512,\n","        nhead: int = 8,\n","        num_encoder_layers: int = 6,\n","        num_decoder_layers: int = 6,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","        pad_id: int = 0,\n","        share_embeddings: bool = False,\n","    ):\n","        super().__init__()\n","        self.pad_id = pad_id\n","        self.d_model = d_model\n","\n","        # Embeddings\n","        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model, padding_idx=pad_id)\n","        if share_embeddings and (src_vocab_size == tgt_vocab_size):\n","            self.tgt_tok_emb = self.src_tok_emb\n","        else:\n","            self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_id)\n","\n","        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n","\n","        # Core Transformer\n","        # batch_first=True にして (batch, seq, d_model) で統一\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True,\n","            norm_first=False,\n","        )\n","\n","        # Output head\n","        self.generator = nn.Linear(d_model, tgt_vocab_size)\n","\n","        # Optional: tie output with target embedding (よくやる)\n","        # 条件: 同一語彙サイズ & embedding共有したい場合\n","        self.tie_output = False\n","        if share_embeddings and (src_vocab_size == tgt_vocab_size):\n","            self.tie_output = True\n","            self.generator.weight = self.tgt_tok_emb.weight\n","\n","    def encode(self, src_tokens: torch.Tensor, src_key_padding_mask: torch.Tensor | None = None):\n","        # (batch, src_len) -> (batch, src_len, d_model)\n","        src = self.src_tok_emb(src_tokens) * math.sqrt(self.d_model)\n","        src = self.pos_enc(src)\n","        memory = self.transformer.encoder(\n","            src,\n","            src_key_padding_mask=src_key_padding_mask\n","        )\n","        return memory\n","\n","    def decode(\n","        self,\n","        tgt_tokens: torch.Tensor,\n","        memory: torch.Tensor,\n","        tgt_mask: torch.Tensor | None = None,\n","        tgt_key_padding_mask: torch.Tensor | None = None,\n","        memory_key_padding_mask: torch.Tensor | None = None,\n","    ):\n","        tgt = self.tgt_tok_emb(tgt_tokens) * math.sqrt(self.d_model)\n","        tgt = self.pos_enc(tgt)\n","        out = self.transformer.decoder(\n","            tgt,\n","            memory,\n","            tgt_mask=tgt_mask,\n","            tgt_key_padding_mask=tgt_key_padding_mask,\n","            memory_key_padding_mask=memory_key_padding_mask,\n","        )\n","        return out\n","\n","    def forward(self, src_tokens: torch.Tensor, tgt_tokens: torch.Tensor):\n","        \"\"\"\n","        src_tokens: (batch, src_len)\n","        tgt_tokens: (batch, tgt_len)  ※ teacher forcing用(入力側)\n","        returns logits: (batch, tgt_len, tgt_vocab_size)\n","        \"\"\"\n","        device = src_tokens.device\n","\n","        src_pad = make_padding_mask(src_tokens, self.pad_id)  # (batch, src_len)\n","        tgt_pad = make_padding_mask(tgt_tokens, self.pad_id)  # (batch, tgt_len)\n","\n","        tgt_len = tgt_tokens.size(1)\n","        tgt_mask = generate_square_subsequent_mask(tgt_len, device=device)  # (tgt_len, tgt_len)\n","\n","        memory = self.encode(src_tokens, src_key_padding_mask=src_pad)\n","        dec_out = self.decode(\n","            tgt_tokens,\n","            memory,\n","            tgt_mask=tgt_mask,\n","            tgt_key_padding_mask=tgt_pad,\n","            memory_key_padding_mask=src_pad,\n","        )\n","\n","        logits = self.generator(dec_out)  # (batch, tgt_len, vocab)\n","        return logits\n","\n","\n","# ----------------------------\n","# Quick sanity check\n","# ----------------------------\n","if __name__ == \"__main__\":\n","    B, src_len, tgt_len = 2, 7, 6\n","    src_vocab, tgt_vocab = 1000, 1200\n","    pad_id = 0\n","\n","    model = TransformerSeq2Seq(\n","        src_vocab_size=src_vocab,\n","        tgt_vocab_size=tgt_vocab,\n","        d_model=256,\n","        nhead=8,\n","        num_encoder_layers=3,\n","        num_decoder_layers=3,\n","        dim_feedforward=1024,\n","        dropout=0.1,\n","        pad_id=pad_id,\n","        share_embeddings=False,\n","    )\n","\n","    src = torch.randint(1, src_vocab, (B, src_len))\n","    tgt_in = torch.randint(1, tgt_vocab, (B, tgt_len))\n","\n","    # 例: 末尾だけPADにしてみる\n","    src[:, -1] = pad_id\n","    tgt_in[:, -1] = pad_id\n","\n","    logits = model(src, tgt_in)\n","    print(\"logits:\", logits.shape)  # (B, tgt_len, tgt_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlRcLvlqbcHO","executionInfo":{"status":"ok","timestamp":1766010581533,"user_tz":-540,"elapsed":9562,"user":{"displayName":"零一真","userId":"13547062280887801150"}},"outputId":"2d1b4303-2249-489d-f934-4f03c2c0cef2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["logits: torch.Size([2, 6, 1200])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n"]}]}]}