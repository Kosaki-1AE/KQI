{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtqpE8rxjNDEdpnygJNbM0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"fku97dSdeVHT","executionInfo":{"status":"ok","timestamp":1766011514222,"user_tz":-540,"elapsed":4787,"user":{"displayName":"零一真","userId":"13547062280887801150"}},"outputId":"170e57ed-bb4f-441d-d4b8-ce108562db13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cpu)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"]}],"source":["!pip install transformers accelerate sentencepiece einops"]},{"cell_type":"code","source":["import math\n","from dataclasses import dataclass\n","from enum import Enum, auto\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Action(Enum):\n","    STAY_STILLNESS = auto()   # その層に留まる（保留）\n","    GO_NEXT = auto()          # 次の層へ進む\n","    GO_BACK = auto()          # 1つ戻る（再初期化の一部）\n","    ASK_CLARIFY = auto()      # 聞き直し（外部に質問）\n","    DECODE = auto()           # 出力してよい（生成へ）\n","\n","@dataclass\n","class ControllerConfig:\n","    # 「迷い」指標（logits entropy）\n","    ent_high: float = 6.0     # これ以上なら迷いすぎ →止まる/聞き直し\n","    ent_low: float = 3.0      # これ以下なら十分自信 → decode許可候補\n","\n","    # 「落ち着き」指標（Δh：層間の変化量）\n","    dh_small: float = 0.20    # これ以下なら落ち着いた（Stillness成立）\n","    dh_large: float = 0.80    # これ以上なら暴れてる（戻る/止まる）\n","\n","    # 行動の優先度・安全装置\n","    max_stay_steps: int = 3   # 止まり続けたら聞き直しへ\n","    allow_decode: bool = True\n","\n","class GSMCController:\n","    \"\"\"\n","    metrics を見て action を返すコントローラ。\n","    - entropy（必須級）：迷い度\n","    - delta_h（任意）：落ち着き度（層変化）\n","    - step_in_layer（任意）：同じ層に留まった回数\n","    \"\"\"\n","    def __init__(self, cfg: ControllerConfig | None = None):\n","        self.cfg = cfg or ControllerConfig()\n","\n","    def decide(self, metrics: dict) -> Action:\n","        \"\"\"\n","        metrics例:\n","          {\n","            \"entropy\": float or torch.Tensor scalar,\n","            \"delta_h\": float or torch.Tensor scalar (optional),\n","            \"step_in_layer\": int (optional),\n","            \"phase\": \"stillness\" | \"coherence\" | \"motion\" | ... (optional)\n","          }\n","        \"\"\"\n","        ent = metrics.get(\"entropy\", None)\n","        dh  = metrics.get(\"delta_h\", None)\n","        stay_steps = int(metrics.get(\"step_in_layer\", 0))\n","        phase = metrics.get(\"phase\", None)\n","\n","        # tensor -> float\n","        if isinstance(ent, torch.Tensor):\n","            ent = float(ent.detach().cpu().item())\n","        if isinstance(dh, torch.Tensor):\n","            dh = float(dh.detach().cpu().item())\n","\n","        # 0) entropy無いと判断できないので保留\n","        if ent is None:\n","            return Action.STAY_STILLNESS\n","\n","        # 1) 迷いが高すぎる：止まる or 聞き直し\n","        if ent >= self.cfg.ent_high:\n","            if stay_steps >= self.cfg.max_stay_steps:\n","                return Action.ASK_CLARIFY\n","            return Action.STAY_STILLNESS\n","\n","        # 2) Δhが取れている場合の安全装置\n","        if dh is not None:\n","            # 暴れてる：戻る or 止める\n","            if dh >= self.cfg.dh_large:\n","                return Action.GO_BACK\n","            # 落ち着いた：次へ進む（Stillness成立）\n","            if dh <= self.cfg.dh_small:\n","                # Coherenceフェーズなら decode 判定へ寄せる\n","                if phase == \"coherence\" and self.cfg.allow_decode and ent <= self.cfg.ent_low:\n","                    return Action.DECODE\n","                return Action.GO_NEXT\n","\n","        # 3) entropyが十分低い（自信がある）なら decode（ただし許可制）\n","        if self.cfg.allow_decode and ent <= self.cfg.ent_low:\n","            # phaseが指定されてるなら coherence でのみ decode するのが安全\n","            if phase is None or phase == \"coherence\":\n","                return Action.DECODE\n","            # coherence以外なら一旦次へ（出力は保留）\n","            return Action.GO_NEXT\n","\n","        # 4) 中間：とりあえず次へ\n","        return Action.GO_NEXT\n","\n","def last_token_entropy(logits: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n","    # logits: (B, T, V)\n","    p = torch.softmax(logits[:, -1, :], dim=-1)          # (B, V)\n","    ent = -(p * (p + eps).log()).sum(dim=-1)             # (B,)\n","    return ent.mean()  # とりまバッチ平均（scalar）\n","\n","def token_entropy_from_logits(logits: torch.Tensor, eps: float = 1e-9):\n","    p = torch.softmax(logits, dim=-1)\n","    ent = -(p * (p + eps).log()).sum(dim=-1)   # (B, T)\n","    return ent\n","\n","# ----------------------------\n","# Positional Encoding　経験則をまんま持ってきたものがこれ。ようやくできるようになったわ。\n","# ----------------------------\n","class PositionalEncoding(nn.Module): #意味のある位置情報を埋め込みに追加する\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","\n","        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # (max_len, 1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)  # even\n","        pe[:, 1::2] = torch.cos(position * div_term)  # odd\n","\n","        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (batch, seq_len, d_model)\n","        \"\"\"\n","        seq_len = x.size(1)\n","        x = x + self.pe[:, :seq_len, :]\n","        return self.dropout(x)\n","\n","\n","# ----------------------------\n","# Masks\n","# ----------------------------\n","def generate_square_subsequent_mask(sz: int, device=None) -> torch.Tensor: #注目すべき未来の情報を隠すためのマスク\n","    \"\"\"\n","    Causal mask for decoder self-attn.\n","    Returns (sz, sz) where upper triangle is -inf.\n","    \"\"\"\n","    device = device or torch.device(\"cpu\")\n","    mask = torch.full((sz, sz), float(\"-inf\"), device=device)\n","    mask = torch.triu(mask, diagonal=1)\n","    return mask\n","\n","def make_padding_mask(tokens: torch.Tensor, pad_id: int) -> torch.Tensor:\n","    \"\"\"\n","    tokens: (batch, seq_len)\n","    returns: (batch, seq_len) True where PAD\n","    \"\"\"\n","    return tokens.eq(pad_id)\n","\n","def layer_delta_h(hiddens: list[torch.Tensor]) -> torch.Tensor:\n","    \"\"\"\n","    hiddens: list of (B, L, D)\n","    returns: scalar tensor（平均Δh）\n","    \"\"\"\n","    if len(hiddens) < 2:\n","        return torch.tensor(0.0, device=hiddens[0].device)\n","    dh = []\n","    for i in range(1, len(hiddens)):\n","        # (B,L,D) -> まずDのノルム → (B,L) → 平均\n","        d = (hiddens[i] - hiddens[i-1]).norm(p=2, dim=-1).mean()\n","        dh.append(d)\n","    return torch.stack(dh).mean()\n","\n","def train_step(model, optimizer, src, tgt, pad_id=0):\n","    \"\"\"\n","    src: (B, S)\n","    tgt: (B, T)  ※BOS込みの想定でも、無い想定でもOK（ここではシンプルにランダム）\n","    \"\"\"\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # teacher forcing 用に 1個ずらす\n","    tgt_in  = tgt[:, :-1]   # (B, T-1)\n","    tgt_out = tgt[:, 1:]    # (B, T-1)\n","\n","    logits = model(src, tgt_in)  # (B, T-1, V)\n","\n","    # PADはlossから除外\n","    loss = F.cross_entropy(\n","        logits.reshape(-1, logits.size(-1)),\n","        tgt_out.reshape(-1),\n","        ignore_index=pad_id\n","    )\n","\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 任意だけど安定する\n","    optimizer.step()\n","    return loss.item(), logits\n","\n","@torch.no_grad()\n","def measure_metrics(model, src, tgt_in, pad_id=0):\n","    model.eval()\n","\n","    device = src.device\n","    src_pad = make_padding_mask(src, pad_id)\n","    tgt_pad = make_padding_mask(tgt_in, pad_id)\n","    tgt_mask = generate_square_subsequent_mask(tgt_in.size(1), device=device)\n","\n","    memory, _ = model.encode_with_layers(src, src_key_padding_mask=src_pad)\n","    dec_out, dec_h = model.decode_with_layers(\n","        tgt_in, memory,\n","        tgt_mask=tgt_mask,\n","        tgt_key_padding_mask=tgt_pad,\n","        memory_key_padding_mask=src_pad,\n","    )\n","\n","    logits = model.generator(dec_out)\n","    ent_last_mean = last_token_entropy(logits)      # scalar tensor\n","    dh_dec = layer_delta_h(dec_h)                   # scalar tensor\n","    return float(ent_last_mean), float(dh_dec), logits.shape\n","\n","# ----------------------------\n","# Full Transformer (Embedding -> Encoder -> Decoder -> Linear)\n","# ----------------------------\n","class TransformerSeq2Seq(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size: int,\n","        tgt_vocab_size: int,\n","        d_model: int = 512,\n","        nhead: int = 8,\n","        num_encoder_layers: int = 6,\n","        num_decoder_layers: int = 6,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","        pad_id: int = 0,\n","        share_embeddings: bool = False,\n","    ):\n","        super().__init__()\n","        self.pad_id = pad_id\n","        self.d_model = d_model\n","\n","        # Embeddings\n","        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model, padding_idx=pad_id)\n","        if share_embeddings and (src_vocab_size == tgt_vocab_size):\n","            self.tgt_tok_emb = self.src_tok_emb\n","        else:\n","            self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_id)\n","\n","        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n","\n","        # ---------\n","        # Encoder/Decoder layers (batch_first=True)\n","        # ---------\n","        self.encoder_layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(\n","                d_model=d_model,\n","                nhead=nhead,\n","                dim_feedforward=dim_feedforward,\n","                dropout=dropout,\n","                batch_first=True,\n","                norm_first=False,\n","            )\n","            for _ in range(num_encoder_layers)\n","        ])\n","        self.encoder_norm = nn.LayerNorm(d_model)\n","\n","        self.decoder_layers = nn.ModuleList([\n","            nn.TransformerDecoderLayer(\n","                d_model=d_model,\n","                nhead=nhead,\n","                dim_feedforward=dim_feedforward,\n","                dropout=dropout,\n","                batch_first=True,\n","                norm_first=False,\n","            )\n","            for _ in range(num_decoder_layers)\n","        ])\n","        self.decoder_norm = nn.LayerNorm(d_model)\n","\n","        # Output head\n","        self.generator = nn.Linear(d_model, tgt_vocab_size)\n","\n","        # Optional: tie output with target embedding\n","        self.tie_output = False\n","        if share_embeddings and (src_vocab_size == tgt_vocab_size):\n","            self.tie_output = True\n","            self.generator.weight = self.tgt_tok_emb.weight\n","\n","    # ---- helpers ----\n","    def _embed_src(self, src_tokens: torch.Tensor) -> torch.Tensor:\n","        src = self.src_tok_emb(src_tokens) * math.sqrt(self.d_model)\n","        return self.pos_enc(src)\n","\n","    def _embed_tgt(self, tgt_tokens: torch.Tensor) -> torch.Tensor:\n","        tgt = self.tgt_tok_emb(tgt_tokens) * math.sqrt(self.d_model)\n","        return self.pos_enc(tgt)\n","\n","    # ---- with layer traces ----\n","    def encode_with_layers(self, src_tokens: torch.Tensor, src_key_padding_mask: torch.Tensor | None = None):\n","        \"\"\"\n","        returns:\n","          memory: (B, S, D)\n","          enc_hiddens: list[(B,S,D)]  各層の出力\n","        \"\"\"\n","        x = self._embed_src(src_tokens)\n","        enc_hiddens = []\n","        for layer in self.encoder_layers:\n","            x = layer(x, src_key_padding_mask=src_key_padding_mask)\n","            enc_hiddens.append(x)\n","        x = self.encoder_norm(x)\n","        return x, enc_hiddens\n","\n","    def decode_with_layers(\n","        self,\n","        tgt_tokens: torch.Tensor,\n","        memory: torch.Tensor,\n","        tgt_mask: torch.Tensor | None = None,\n","        tgt_key_padding_mask: torch.Tensor | None = None,\n","        memory_key_padding_mask: torch.Tensor | None = None,\n","    ):\n","        \"\"\"\n","        returns:\n","          out: (B, T, D)\n","          dec_hiddens: list[(B,T,D)] 各層の出力\n","        \"\"\"\n","        y = self._embed_tgt(tgt_tokens)\n","        dec_hiddens = []\n","        for layer in self.decoder_layers:\n","            y = layer(\n","                y,\n","                memory,\n","                tgt_mask=tgt_mask,\n","                tgt_key_padding_mask=tgt_key_padding_mask,\n","                memory_key_padding_mask=memory_key_padding_mask,\n","            )\n","            dec_hiddens.append(y)\n","        y = self.decoder_norm(y)\n","        return y, dec_hiddens\n","\n","    # ---- original APIs (keep) ----\n","    def encode(self, src_tokens: torch.Tensor, src_key_padding_mask: torch.Tensor | None = None):\n","        memory, _ = self.encode_with_layers(src_tokens, src_key_padding_mask=src_key_padding_mask)\n","        return memory\n","\n","    def decode(\n","        self,\n","        tgt_tokens: torch.Tensor,\n","        memory: torch.Tensor,\n","        tgt_mask: torch.Tensor | None = None,\n","        tgt_key_padding_mask: torch.Tensor | None = None,\n","        memory_key_padding_mask: torch.Tensor | None = None,\n","    ):\n","        out, _ = self.decode_with_layers(\n","            tgt_tokens, memory,\n","            tgt_mask=tgt_mask,\n","            tgt_key_padding_mask=tgt_key_padding_mask,\n","            memory_key_padding_mask=memory_key_padding_mask,\n","        )\n","        return out\n","\n","    def forward(self, src_tokens: torch.Tensor, tgt_tokens: torch.Tensor):\n","        device = src_tokens.device\n","\n","        src_pad = make_padding_mask(src_tokens, self.pad_id)  # (B,S)\n","        tgt_pad = make_padding_mask(tgt_tokens, self.pad_id)  # (B,T)\n","\n","        tgt_len = tgt_tokens.size(1)\n","        tgt_mask = generate_square_subsequent_mask(tgt_len, device=device)  # (T,T)\n","\n","        memory = self.encode(src_tokens, src_key_padding_mask=src_pad)\n","        dec_out = self.decode(\n","            tgt_tokens,\n","            memory,\n","            tgt_mask=tgt_mask,\n","            tgt_key_padding_mask=tgt_pad,\n","            memory_key_padding_mask=src_pad,\n","        )\n","        logits = self.generator(dec_out)\n","        return logits\n","\n","# ----------------------------\n","# Quick sanity check\n","# ----------------------------\n","if __name__ == \"__main__\":\n","    B, src_len, tgt_len = 2, 7, 6\n","    src_vocab, tgt_vocab = 1000, 1200\n","    pad_id = 0\n","\n","    model = TransformerSeq2Seq(\n","        src_vocab_size=src_vocab,\n","        tgt_vocab_size=tgt_vocab,\n","        d_model=256,\n","        nhead=8,\n","        num_encoder_layers=3,\n","        num_decoder_layers=3,\n","        dim_feedforward=1024,\n","        dropout=0.1,\n","        pad_id=pad_id,\n","        share_embeddings=False,\n","    )\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n","    controller = GSMCController()\n","\n","    src = torch.randint(1, src_vocab, (B, src_len))\n","    tgt_in = torch.randint(1, tgt_vocab, (B, tgt_len))\n","    src[:, -1] = pad_id\n","    tgt_in[:, -1] = pad_id\n","\n","    # ===== 固定の観測用データ（probe）を作る =====\n","    probe_src = torch.randint(1, src_vocab, (B, src_len))\n","    probe_tgt = torch.randint(1, tgt_vocab, (B, tgt_len))\n","    probe_src[:, -1] = pad_id\n","    probe_tgt[:, -1] = pad_id\n","    probe_tgt_in = probe_tgt[:, :-1]  # decoder入力用\n","\n","    # ===== 学習前の基準値 =====\n","    base_ent, base_dh, shape0 = measure_metrics(model, probe_src, probe_tgt_in, pad_id)\n","    base_action = controller.decide({\"entropy\": base_ent, \"delta_h\": base_dh, \"phase\": \"coherence\", \"step_in_layer\": 0})\n","    print(f\"[BASE] logits={shape0} ent={base_ent:.3f} dh={base_dh:.3f} action={base_action}\")\n","\n","    # ===== 学習ループ =====\n","    for step in range(1, 201):\n","        # 学習用のランダムデータ（ここは後で本物データに置き換えればOK）\n","        src = torch.randint(1, src_vocab, (B, src_len))\n","        tgt = torch.randint(1, tgt_vocab, (B, tgt_len))\n","        src[:, -1] = pad_id\n","        tgt[:, -1] = pad_id\n","\n","        loss, _ = train_step(model, optimizer, src, tgt, pad_id=pad_id)\n","\n","        # ===== 10ステップごとに “同じprobe” で現在値を観測 =====\n","        if step % 10 == 0:\n","            cur_ent, cur_dh, shape = measure_metrics(model, probe_src, probe_tgt_in, pad_id)\n","            cur_action = controller.decide({\"entropy\": cur_ent, \"delta_h\": cur_dh, \"phase\": \"coherence\", \"step_in_layer\": 0})\n","\n","            print(\n","                f\"[STEP {step:03d}] loss={loss:.3f} \"\n","                f\"ent={cur_ent:.3f} (Δ{cur_ent-base_ent:+.3f}) \"\n","                f\"dh={cur_dh:.3f} (Δ{cur_dh-base_dh:+.3f}) \"\n","                f\"action={cur_action}\"\n","            )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oq7xQp9UeY2d","executionInfo":{"status":"ok","timestamp":1766011545740,"user_tz":-540,"elapsed":31506,"user":{"displayName":"零一真","userId":"13547062280887801150"}},"outputId":"cd601d80-0743-4286-d9f3-99ba049ccfb7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[BASE] logits=torch.Size([2, 5, 1200]) ent=6.915 dh=6.756 action=Action.STAY_STILLNESS\n","[STEP 010] loss=7.364 ent=6.887 (Δ-0.028) dh=9.034 (Δ+2.279) action=Action.STAY_STILLNESS\n","[STEP 020] loss=7.256 ent=6.830 (Δ-0.085) dh=9.826 (Δ+3.070) action=Action.STAY_STILLNESS\n","[STEP 030] loss=7.383 ent=6.730 (Δ-0.185) dh=10.229 (Δ+3.473) action=Action.STAY_STILLNESS\n","[STEP 040] loss=7.474 ent=6.683 (Δ-0.232) dh=10.479 (Δ+3.723) action=Action.STAY_STILLNESS\n","[STEP 050] loss=7.464 ent=6.679 (Δ-0.236) dh=10.668 (Δ+3.912) action=Action.STAY_STILLNESS\n","[STEP 060] loss=7.747 ent=6.686 (Δ-0.229) dh=11.036 (Δ+4.280) action=Action.STAY_STILLNESS\n","[STEP 070] loss=7.565 ent=6.711 (Δ-0.205) dh=11.497 (Δ+4.741) action=Action.STAY_STILLNESS\n","[STEP 080] loss=7.040 ent=6.781 (Δ-0.134) dh=11.964 (Δ+5.208) action=Action.STAY_STILLNESS\n","[STEP 090] loss=7.241 ent=6.814 (Δ-0.101) dh=12.030 (Δ+5.274) action=Action.STAY_STILLNESS\n","[STEP 100] loss=7.521 ent=6.763 (Δ-0.152) dh=11.960 (Δ+5.204) action=Action.STAY_STILLNESS\n","[STEP 110] loss=6.943 ent=6.798 (Δ-0.117) dh=12.160 (Δ+5.404) action=Action.STAY_STILLNESS\n","[STEP 120] loss=7.317 ent=6.726 (Δ-0.189) dh=11.969 (Δ+5.213) action=Action.STAY_STILLNESS\n","[STEP 130] loss=7.392 ent=6.695 (Δ-0.220) dh=11.740 (Δ+4.984) action=Action.STAY_STILLNESS\n","[STEP 140] loss=7.293 ent=6.760 (Δ-0.155) dh=11.863 (Δ+5.107) action=Action.STAY_STILLNESS\n","[STEP 150] loss=7.457 ent=6.743 (Δ-0.172) dh=11.967 (Δ+5.211) action=Action.STAY_STILLNESS\n","[STEP 160] loss=7.311 ent=6.817 (Δ-0.099) dh=12.155 (Δ+5.399) action=Action.STAY_STILLNESS\n","[STEP 170] loss=7.743 ent=6.803 (Δ-0.112) dh=11.962 (Δ+5.206) action=Action.STAY_STILLNESS\n","[STEP 180] loss=7.303 ent=6.764 (Δ-0.151) dh=11.653 (Δ+4.897) action=Action.STAY_STILLNESS\n","[STEP 190] loss=7.717 ent=6.779 (Δ-0.136) dh=11.721 (Δ+4.965) action=Action.STAY_STILLNESS\n","[STEP 200] loss=7.121 ent=6.800 (Δ-0.115) dh=11.804 (Δ+5.048) action=Action.STAY_STILLNESS\n"]}]}]}