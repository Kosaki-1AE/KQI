import torch
import torch.nn as nn
import torch.optim as optim
from improvformer import ImprovFormer, visualize_attention
from torch.utils.data import DataLoader, TensorDataset

# ä»®ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
batch_size = 8
seq_len = 10
input_dim = 51
output_dim = 10
num_classes = 4
x_data = torch.randn(100, seq_len, input_dim)
y_data = torch.randn(100, seq_len, output_dim)
labels = torch.randint(0, num_classes, (100,))

dataset = TensorDataset(x_data, y_data)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

# ãƒ¢ãƒ‡ãƒ«æº–å‚™
model = ImprovFormer(input_dim=input_dim, output_dim=output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# GPUå¯¾å¿œï¼ˆä»»æ„ï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(5):
    total_loss = 0
    for x_batch, y_batch in dataloader:
        optimizer.zero_grad()
        output = model(x_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
torch.save(model.state_dict(), "improvformer_model.pth")

# ğŸ’¡ Attentionå¯è¦–åŒ–ï¼ˆæœ€å¾Œã®ãƒãƒƒãƒã‚’ä¾‹ã«ï¼‰
print("\nğŸ§  Self-Attention å¯è¦–åŒ–ä¸­...")
model.eval()
with torch.no_grad():
    x_sample = x_data[:1].to(device)
    output, attn = model(x_sample, return_attention=True)
    visualize_attention()