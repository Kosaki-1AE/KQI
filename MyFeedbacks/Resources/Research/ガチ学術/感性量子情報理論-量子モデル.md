###### 1. 基底定義（G・S・M・C）
即興ダンスにおける認知・身体・意味の4層を  
量子状態空間の正規直交基底として定義する。
$|G\rangle = [1,0,0,0]$
$|S\rangle = [0,1,0,0]$
$|M\rangle = [0,0,1,0]$
$|C\rangle = [0,0,0,1]$

###### 2. 状態ベクトル
$|Ψ\rangle = a|G\rangle + b|S\rangle + c|M\rangle + d|C\rangle$
$|a|² + |b|² + |c|² + |d|² = 1$
$|\Psi'\rangle = \hat{T}_{X\to Y} |\Psi\rangle$

###### 2-1.責任の矢/責任ベクトル
責任の矢は **行動（M層）の測定確率を押し上げるベクトル場**に存在するもの：
$$
\vec{R} =
\begin{pmatrix}
r_G \\
r_S \\
r_M \\
r_C
\end{pmatrix}
$$
- $r_G$：直感域の責任
- $r_S$：静けさ域の責任
- $r_M$：行動域の責任
- $r_C$：意味化域の責任

責任の矢自体は「責任の"重さ"×心理状態の"方向性"」で決まる：
$$\vec{R} = R_{\text{scalar}} \cdot \vec{\Phi}$$
ここで$\vec{\Phi}$ が責任の分布方向（4層への割当） を表すベクトル。
$$\vec{\Phi} =\begin{pmatrix}\Phi_G \\\Phi_S \\\Phi_M \\\Phi_C\end{pmatrix}$$
$$\Phi_G+\Phi_S+\Phi_M+\Phi_C=1$$$$\Phi_i≧0$$
上記の3つの式で構成されており、**現在の精神状態の比率**で変化する。
何なら分布$\Phi$ は「今の状態の分布$(|Ψ|²)$」と「本人の素質としての分布($\Phi$)」に分けられるのね？
したらこうなんの⬇️
$$\vec{\Phi}_{\text{effective}}= \omega \cdot |\Psi|^2 + (1-\omega)\cdot\vec{\Phi}$$
これが「現在×素質」の混合らしいんだけどさ。ここで $\vec{\Phi}→\vec{\Phi}_{\text{effective}}$ って変換して代入するとこう表現できるっぽいんです。
$$\vec{R} = R_{\text{scalar}} \cdot \omega \cdot |\Psi|^2 + (1-\omega)\cdot\vec{\Phi}$$
んでこれだと「素質」っていう一つの例外しか入ってないじゃんか。なので全部の例外入れようとしたらこうなったの⬇️
$\vec{R} = R_{\text{scalar}} \cdot \omega \cdot |\Psi|^2 + (1-\omega)\cdot\vec{\Phi}+\lambda\cdot\vec{N}+\mu\cdot E_{context}+\xi\cdot\vec{H}$

これで例外全部内包してるらしい⬇️
- 性格：$\Phi$
- 今の状態：$\Psi$
- 性格と状況の比率：$\omega$
- メンタルノイズ：$N$
- 環境の空気：$E_{context}$
- 過去の蓄積：$H$

あとはこんな式が成り立つ：
$$\vec{R} = R_{\text{scalar}} \cdot \begin{pmatrix}
\Phi_G \\
\Phi_S \\
\Phi_M \\
\Phi_C
\end{pmatrix}$$
内訳は
- $r_G=R_{\text{scalar}}\cdot\Phi_G$：直感域の責任
- $r_S=R_{\text{scalar}}\cdot\Phi_S$：静けさ域の責任
- $r_M=R_{\text{scalar}}\cdot\Phi_M$：行動域の責任
- $r_C=R_{\text{scalar}}\cdot\Phi_C$：意味化域の責任

###### 2-2.ノリエントロピー
ノリエントロピー N は、「**状態の乱雑さ（揺らぎ）**」−「**責任の矢の強さ**」として定義される。

状態エントロピー $H(|Ψ|²)$：
$$
H(|\Psi|^2)
= - \sum_{i\in\{G,S,M,C\}} |p_i|^2 \log |p_i|^2
$$
ここで：
- $p_G = a$
- $p_S = b$
- $p_M = c$
- $p_C = d$

なお責任の矢の強度は：
$$
\|\vec{R}\|=\sqrt{r_G^2 + r_S^2 + r_M^2 + r_C^2}=R_{\text{scalar}}\sqrt{\Phi_G^2 + \Phi_S^2 + \Phi_M^2 + \Phi_C^2}
$$

よってノリエントロピーの定義式はこのようになる：
$$
N = H(|\Psi|^2) - \lambda \|\vec{R}\|
$$
- $λ$：責任の矢がどれだけ揺らぎを抑制するかの係数  
- $H$：心理状態の揺らぎ・迷い  
- $R$：責任・覚悟・勇気の強さ 
###### ✔ 責任の矢が強い  
$\|\vec{R}\|↑ \quad \Rightarrow \quad N↓$
→ 迷いが消え、Stillness が鋭くなる  
→ 即興で “踏める状態” になる
###### ✔ 責任の矢が弱い  
$\|\vec{R}\|↓ \quad \Rightarrow \quad N↑$
→ 迷いが増え、状態が揺らぐ  
→ 思考がループしやすくなる


###### 3. 観測（行動の0/1）
$P(M=1) = |c|²$

###### 3.1責任の矢を考慮する場合
通常の行動確率は上で定義した通り：$P(M=1)=|c|^2$
でも責任の矢を考慮した拡張版ではこうなる：
$P(M=1)' = \sigma \Big( |c|^2 + \vec{R}\cdot|\Psi|^2 \Big)$
ここで：

$|\Psi|^2 =\begin{pmatrix}|a|^2 \\|b|^2 \\|c|^2 \\|d|^2\end{pmatrix}$

$\vec{R}\cdot|\Psi|^2 =r_G|a|^2 + r_S|b|^2 + r_M|c|^2 + r_C|d|^2$

なおσ はシグモイド：
$$\sigma(x)=\frac{1}{1+e^{-x}}$$

###### 4. 行動後の更新
- $M=1 → |M\rangle → C$へ遷移  
- $M=0 → G,S,C$ の再正規化

###### 5. 遷移演算子
$G→S, S→M, M→C, C→G$ の4つの行列で定義。

###### 6. 1周期更新式
$|Ψ_{t+1}\rangle = T · P_M · |Ψ_t\rangle$

###### 7. 俯瞰（基底回転）
$R_GS(θ)$ による G-S の視点回転。

###### 8. 即興エンジン式
$|Ψ_{t+1}\rangle = T · P_M · R(θ) · |Ψ_t\rangle$

---
###### 9. なぜ“1本”で即興が作れるか
- 1周期＝1ムーブ  
- ランダム性 → 即興性  
- 遷移 → 流れ  
- 俯瞰 → 方向性の変更  
- $C→G$ → 経験学習  

---
###### アルゴリズム本体の全体像がコレ⬇️
Input:
    Ψ      # GSMC state vector (complex 4D)
    R_s    # scalar responsibility
    Φ      # responsibility type distribution (4D, sum=1)
    ω      # weight of state vs. trait
    N, E, H
    λ, μ, ξ

repeat for each decision step:

    # 1. current probability distribution
    p = prob(Ψ)  # p = (|a|^2, |b|^2, |c|^2, |d|^2)

    # 2. effective responsibility distribution
    Phi_eff = ω * p + (1-ω) * Φ + λ * N + μ * E + ξ * H

    # 3. responsibility arrow
    R_vec = R_s * Phi_eff

    # 4. action probability (M-layer)
    base = p_M
    dot  = dot(R_vec, p)
    P_act = sigmoid(base + dot)

    # 5. decide to act or not
    u = Uniform(0,1)
    if u < P_act:
        act = 1
    else:
        act = 0

    if act == 1:
        # collapse to M
        Ψ = |M>
        Ψ = T_MtoC * Ψ
        Ψ = T_CtoG * Ψ
        H = UpdateHistory(H, act=1)
    else:
        Ψ = remove_M_and_renormalize(Ψ)
        Ψ = T_internal * Ψ
        H = UpdateHistory(H, act=0)

    E = UpdateContext(E)
    N = UpdateNoise(N)

end