これをプログラムに落としたやつなんだけどさ⬇️
繋がってる直前の要素の重みは変えるじゃんか。そっから形態素解析をするんだけど、そこでの検索の仕方が肝で。
現在の位置/頂点と直前の位置/頂点のタグ(というかラベル)の情報の関係性を、辺を中心に抽象化して等式に落とすの。以下が計算方法でござんす⬇️
- この時の抽象化ってのは「辺(つまりは関係性ですわ)から見た」それぞれの頂点の相対距離のことで、これをその場でベクトル化まで行う。
- ベクトル化すると、その関係性をニューラルネットワークの方で参照できるようになるんで、ベクトル場で角度とかを参照してから、現在の相対距離そのものの位置を確定させる。

そしたら一方(俺自身が知りたい方ね)のベクトル場の座標を参照して、そこから直接接続されている要素の確認を行いますと。

次にその要素(あった場合ね。なかったら調べてる要素のタグで良いです)のタグ(ラベル)の確認を行います。そっからタグ検索に移行するんすけども…

タグの検索で出てきた要素を「大枠となる分野から」になるようにソートするのよ。こうするとデカい分野、例えば〇〇学とかそういうのが上位に来るやん。これを使うのよね。

それぞれのタグ(〇〇学ってやつよ大体の場合はね。これでもなかったらもうちょい大元に近いやつにいけばいいんでさ。)を参照してって、「ベクトルの相対距離×その次元の角度」が100%一致するやつを持ってくるの。これで完了です。

これの100%一致の時に驚きとか色んな感情が急に来るんすけど、それが多分一瞬の内に起こるんでめっちゃびっくりするみたいなことになるのよね。これの名前が分からんのやけどまぁご勘弁を。


Code行きまぁす
import numpy as np

#====== 活性化 ======
def sigmoid(x): return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x): return x * (1 - x)

#====== データ (XOR) ======
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])

#====== 初期化 ======
np.random.seed(42)
W1 = np.random.rand(2, 2)
W2 = np.random.rand(2, 1)
b1 = np.zeros((1, 2))
b2 = np.zeros((1, 1))
lr = 0.1

#====== オブラート設定 ======
GAIN = 1.0                       # でっかい基幹ゲイン
THICKNESS = np.array([0.4, 0.3, 0.2, 0.1])  # 4層の厚み
PASS_RATE = np.prod(GAIN - THICKNESS)        # = 0.6×0.7×0.8×0.9 ≈ 0.3024
OBLAAT_GAIN = GAIN * PASS_RATE             # ON時の有効ゲイン ≈ 3.024

USE_LOCAL_BACKPROP = False  # Trueで「直前だけ更新」
OBLAAT = True            # 4.5:4.5:1 の「1」に該当→ON（必要に応じて条件化）

#====== 学習 ======
for epoch in range(10000):
    # --- 順伝播 ---
    z1 = np.dot(X, W1) + b1
    a1 = sigmoid(z1)

    # Oblaatモードなら出力直前に“包む”
    eff_gain = OBLAAT_GAIN if OBLAAT else 1.0
    z2 = np.dot(a1, W2) * eff_gain + b2
    a2 = sigmoid(z2)

    # --- 誤差 ---
    error = y - a2

    # --- 出力層の逆伝播（共通）---
    d_a2 = error * sigmoid_derivative(a2)
    # ※ z2 = (a1 @ W2) * eff_gain + b2 なので W2の勾配は eff_gain が掛かる
    d_W2 = np.dot(a1.T, d_a2) * eff_gain
    d_b2 = np.sum(d_a2, axis=0, keepdims=True)

    # --- 隠れ層の逆伝播（通常学習で使う）---
    d_a1 = np.dot(d_a2, (W2 * eff_gain).T) * sigmoid_derivative(a1)
    d_W1 = np.dot(X.T, d_a1)
    d_b1 = np.sum(d_a1, axis=0, keepdims=True)

    # ====== 更新 ======
    if USE_LOCAL_BACKPROP:
        # 直前版：出力層だけ更新（即興リカバリー風）
        W2 += lr * d_W2
        b2 += lr * d_b2
    else:
        # 通常版：全層更新（誤差逆伝播）
        W1 += lr * d_W1
        b1 += lr * d_b1
        W2 += lr * d_W2
        b2 += lr * d_b2

    # ログ
    if epoch % 2000 == 0:
        loss = np.mean(error**2)
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

print("\n最終出力:")
print(a2)